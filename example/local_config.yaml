# Config template to input parameters into dnntime
meta:
    user_interface: 'console'  # either 'console' (default) or 'notebook'
    datetime_column: 'time'
    target_column: 'total load actual'
etl1:
    description: 'Extract Data from Source'
    extract:
        alias: 'Original'   # If alias is blank, then default will be 'Extract' with proper format
    	file_path: 'datasets/kaggle/energy_dataset.csv'
        delineator: ','
    univariate: True
etl2:
    description: 'Preprocessing I (Cleaning)'
    clean:  # this entire block can be omitted if 'auto_clean' is not needed
        time_interval: 'hourly'
        timezone: 'Europe/Madrid'
        allow_negatives: True  # set to Fase if negative values are prohibited, hence set to NaN
        all_numeric: True  # convert entire DataFrame into float64 (if not already done so)
        nan_fill_type: 'linear'  # leave '' to not fill, see: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html
        output_type: 'reg'  # regression 'reg' or classication 'class'
eda1:
    description: 'EDA I (General)'
    general:
        title: 'PJME Regional Power Demand Time-Series'
        x_label: 'Datetime'
        y_label: 'Total Demand (MW)'
eda2:
    description: 'EDA II (Time-Series Stats)'
    statistical:
        title: 'Total Electricity Demand'
        x_label: 'Datetime'
        y_label: 'Total Demand (MW)'
        confidence_interval: 0.95
etl3:
    description: 'Preprocessing II (Transformations)'
    transform1:
        alias: 'Box-Cox Standardized'
        method: 'box-cox'
        standardize: True
    transform2:
        alias: 'Deseasonalize'
        method: 'deseasonalize'
        decomposition_model: 'additive'
etl4:
    description: 'Preprocessing III (Make Supervised)'
    supervise:
        training_period: 'biweek'  # walk-forward validation input timesteps
        forecast_period: 'day'  # walk-forward validation output timesteps
        validation_set:  'biweek'  # size of the validation set, taken from period prior to 
        test_set: 'month'  # size of the test set, taken from the last <input> of dataset
        max_gap: 0.01
model:
    description: 'Model Search (DNNs)'
    enable_gpu: False
    dnn1:
        alias: 'RNN'
        model_type: 'rnn'
        epochs: 2
        batch_size: 512
        number_layers: 3
        number_units: 128   # number of units per layer
        dropout_rate: 0.15   # dropout rate
        optimizer: 'adam'
        objective_function: 'mse'  # the loss value minimized by the dnn model
        verbose: 1     # description for model 'fit' and 'evaluate'
        score_type: 'rmse'
    dnn2:
        alias: 'LSTM'
        model_type: 'lstm'
        epochs: 2
        batch_size: 512
        number_layers: 3
        number_units: 128   # number of units per layer
        dropout_rate: 0.15   # dropout rate
        optimizer: 'adam'
        objective_function: 'mse'  # the loss value minimized by the dnn model
        verbose: 1     # description for model 'fit' and 'evaluate'
        score_type: 'rmse'
    dnn3:
        alias: 'GRU'
        model_type: 'gru'
        epochs: 2
        batch_size: 512
        number_layers: 3
        number_units: 128   # number of units per layer
        dropout_rate: 0.15   # dropout rate
        optimizer: 'adam'
        objective_function: 'mse'  # the loss value minimized by the dnn model
        verbose: 1     # description for model 'fit' and 'evaluate'
        score_type: 'rmse'
    dnn4:
        alias: 'CONVLSTM'
        model_type: 'convlstm'
        epochs: 2
        batch_size: 512
        number_layers: 3
        number_units: 128   # number of units per layer
        dropout_rate: 0.15   # dropout rate
        optimizer: 'adam'
        objective_function: 'mse'  # the loss value minimized by the dnn model
        verbose: 1     # description for model 'fit' and 'evaluate'
        score_type: 'rmse'
